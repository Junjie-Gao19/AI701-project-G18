# AI701-project-G18
## Abstract
With the advancement of large model technology, contemporary mainstream large models exhibit robust reasoning and conversational capabilities. While these technologies bring convenience, they also harbor significant security risks, as they could potentially be manipulated by malicious actors to generate harmful content detrimental to society. Therefore, identifying model security vulnerabilities and mitigating them are paramount. 
This paper proposes a nested prompt approach to hypnotize the model, providing it with virtual scenarios and antagonists to thwart, thereby achieving jailbreaks and generating harmful information.
